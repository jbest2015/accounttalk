{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7f6cac",
   "metadata": {},
   "source": [
    "Account Talk : Program to feed Account data to ChatGpt and allow user to interact with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7070357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "openai.organization = \"org-Jf9H58za3FT6nakElonoxN6Y\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b0bea",
   "metadata": {},
   "source": [
    "Load the account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_cat_all.csv',nrows=90)\n",
    "json_data = df[\"all\"].to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "The api operates for chatgpt operates on a token system.\n",
    "This routine calculates the number of tokens so we can not exceed that amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81385b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "   \n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except KeyError:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "            num_tokens = 0\n",
    "            for message in messages:\n",
    "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "                for key, value in message.items():\n",
    "                    num_tokens += len(encoding.encode(value))\n",
    "                    if key == \"name\":  # if there's a name, the role is omitted\n",
    "                        num_tokens += -1  # role is always required and always 1 token\n",
    "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "            return num_tokens\n",
    "        else:\n",
    "            raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "\n",
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful accountant who provides detailed analysis of transactions\"}]\n",
    "conversation.append({\"role\": \"system\", \"content\": json_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d70ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e851664",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chatgpt_responses.txt\", \"w\") as f:\n",
    "    \n",
    "    while(True):\n",
    "        user_input = input(\"\")     \n",
    "\n",
    "        tokens = num_tokens_from_messages(conversation)\n",
    "        if tokens > 3500:\n",
    "            #conversation = [line for line in conversation if line['role'] != 'user']\n",
    "            conversation = conversation[:2]\n",
    "            print(\"Tokens exceeded 4096\")\n",
    "            #print(conversation)\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        print(f\"{num_tokens_from_messages(conversation)} prompt tokens counted.\")\n",
    "\n",
    "\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = conversation,\n",
    "            temperature=1,\n",
    "            max_tokens=250,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        #conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "        print(\"\\n\" + response['choices'][0]['message']['content'] + \"\\n\")\n",
    "        print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n",
    "    \n",
    "       # Save response to file\n",
    "        f.write(\"User: \" + user_input + \"\\n\")\n",
    "        f.write(\"AI: \" + response['choices'][0]['message']['content'] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49d313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
