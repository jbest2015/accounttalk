{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7f6cac",
   "metadata": {},
   "source": [
    "Account Talk : Program to feed Account data to ChatGpt and allow user to interact with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18283e",
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install openai pandas tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7070357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "\n",
    "openai.api_key = \"\"\n",
    "openai.organization = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b0bea",
   "metadata": {},
   "source": [
    "Load the account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_cat_all.csv',nrows=90)\n",
    "json_data = df[\"all\"].to_json(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621f658",
   "metadata": {},
   "source": [
    "The api operates for chatgpt operates on a token system.\n",
    "This routine calculates the number of tokens so we can not exceed that amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81385b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "   \n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except KeyError:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "            num_tokens = 0\n",
    "            for message in messages:\n",
    "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "                for key, value in message.items():\n",
    "                    num_tokens += len(encoding.encode(value))\n",
    "                    if key == \"name\":  # if there's a name, the role is omitted\n",
    "                        num_tokens += -1  # role is always required and always 1 token\n",
    "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "            return num_tokens\n",
    "        else:\n",
    "            raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5418353",
   "metadata": {},
   "source": [
    "This is where we create the prompt. The prompt system for the new chat gpt-3.5-turbo-0301 can be found at this link. \n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful accountant who provides detailed analysis of transactions\"}]\n",
    "conversation.append({\"role\": \"system\", \"content\": json_data})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "249d70ee",
   "metadata": {},
   "source": [
    "The following code will call openai using the API Key , it creates a input prompt. Once the user fills out the prompt it will then send the request. \n",
    "TODO: The token system currently checks the conversation object and if it is over 3500 (a little less than the 4096 available) it prunes the conversation down to just the core data . \n",
    "        This can cause it to lose the context of the conversation because it will prune all previous requests for example\n",
    "\n",
    "        User: how much did I spend on Amazon? \n",
    "        AI: You spent x.xxx on amazon\n",
    "        User: Please show me the transactions (which should be related to the previous request)\n",
    "        AI: <SHOWS ALL TRANSACTIONS, NOT AMAZON TRANSACTIONS>\n",
    "\n",
    "        This because the context got pruned between the first and second user request.. not sure how to fix.. open to ideas. \n",
    "\n",
    "this code will also create a file called chatgpt_responses.txt that will include all of your calls and requests in case you want to repeat them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e851664",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chatgpt_responses.txt\", \"w\") as f:\n",
    "    \n",
    "    while(True):\n",
    "        user_input = input(\"\")     \n",
    "\n",
    "        tokens = num_tokens_from_messages(conversation)\n",
    "        if tokens > 3500:\n",
    "            #conversation = [line for line in conversation if line['role'] != 'user']\n",
    "            conversation = conversation[:2]\n",
    "            print(\"Tokens exceeded 4096\")\n",
    "            #print(conversation)\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        print(f\"{num_tokens_from_messages(conversation)} prompt tokens counted.\")\n",
    "\n",
    "\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = conversation,\n",
    "            temperature=1,\n",
    "            max_tokens=250,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        #conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "        print(\"\\n\" + response['choices'][0]['message']['content'] + \"\\n\")\n",
    "        print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n",
    "    \n",
    "       # Save response to file\n",
    "        f.write(\"User: \" + user_input + \"\\n\")\n",
    "        f.write(\"AI: \" + response['choices'][0]['message']['content'] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49d313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
